import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime

def crawl_wikipedia_events_zh():
    # ä¸­æ–‡ç»´åŸºç™¾ç§‘æ¯æ—¥å¤§äº‹é¡µé¢
    url = "https://zh.wikipedia.org/wiki/Portal:æ–°é—»åŠ¨æ€"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }

    print(f"ğŸŒ æ­£åœ¨åŒæ­¥ç»´åŸºç™¾ç§‘ã€ä¸­æ–‡ã€‘å…¨çƒä¿¡å·...")
    
    try:
        # ä½¿ç”¨ verify=True æ˜¯é»˜è®¤çš„ï¼Œå¦‚æœé‡åˆ°è¯ä¹¦é—®é¢˜å¯ä»¥å°è¯• False
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        # æ˜¾å¼è®¾ç½®ç¼–ç ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')

        # ä¸­æ–‡ç‰ˆç»´åŸºç™¾ç§‘çš„ç»“æ„ï¼šé€šå¸¸åœ¨ id="mw-content-text" ä¸‹çš„åˆ—è¡¨é¡¹é‡Œ
        events = []
        
        # å¯»æ‰¾æœ€è¿‘çš„æ–°é—»åˆ—è¡¨å†…å®¹
        content = soup.find('div', {'class': 'mw-parser-output'})
        if content:
            # æŠ“å–æœ€è¿‘çš„ li æ ‡ç­¾å†…å®¹ï¼ˆé€šå¸¸æ˜¯æœ€è¿‘çš„æ–°é—»æ¡ç›®ï¼‰
            for li in content.find_all('li'):
                text = li.get_text().strip()
                # è¿‡æ»¤æ‰å¤ªçŸ­çš„æˆ–è€…å¯¼èˆªç±»çš„å¹²æ‰°é¡¹
                if len(text) > 10 and not text.startswith('è¿‘æœŸ'):
                    events.append(text)

        if not events:
            print("âš ï¸ æœªèƒ½æå–åˆ°ä¸­æ–‡æ¡ç›®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦èƒ½è®¿é—® zh.wikipedia.org")
            return

        # å­˜å‚¨
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
        filename = f"wiki_zh_{timestamp}.txt"
        filepath = os.path.join("news_data", filename)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"æ¥æº: ä¸­æ–‡ç»´åŸºç™¾ç§‘ æ–°é—»åŠ¨æ€\n")
            f.write(f"æŠ“å–æ—¶é—´: {datetime.now()}\n")
            f.write("-" * 30 + "\n")
            # åªå–å‰ 15 æ¡æœ€ç›¸å…³çš„ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
            f.write("\n\n".join(events[:15]))

        print(f"âœ… ä¸­æ–‡ä¿¡å·å·²æ•è·: {filename}")

    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤ºï¼šå¦‚æœæŠ¥è¿æ¥é”™è¯¯ï¼Œå¯èƒ½æ˜¯éœ€è¦â€˜æ¢¯å­â€™ã€‚å¦‚æœä¸æƒ³æŠ˜è…¾ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥æ¢æˆâ€˜ç™¾åº¦æ–°é—»â€™ã€‚")

if __name__ == "__main__":
    if not os.path.exists("news_data"):
        os.makedirs("news_data")
    crawl_wikipedia_events_zh()